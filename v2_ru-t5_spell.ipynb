{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4ea846-86d0-4e6e-b1b6-33ab29eed7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (13.9.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.11/site-packages (0.4.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich) (2.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ (–µ—Å–ª–∏ –µ—â—ë –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã)\n",
    "!pip install transformers datasets peft bitsandbytes torch accelerate sentencepiece pandas psutil rich colorama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c364174d-29d3-45e2-a358-4caef6624cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import glob\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, \n",
    "    DataCollatorForSeq2Seq, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# üîπ –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–æ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å `tokenizers`\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f960b0c-7145-4867-8789-d0ffd14c9f31",
   "metadata": {},
   "source": [
    "<b>–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e819146-12de-4909-a0a2-bfccea3aa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_chunk_size(min_chunk_size=10000, max_chunk_size=50000):\n",
    "    \"\"\"\n",
    "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π RAM –∏ VRAM.\n",
    "    \"\"\"\n",
    "    total_ram = psutil.virtual_memory().available / (1024 ** 3)\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if torch.cuda.is_available() else 0\n",
    "    estimated_chunk_size = int((total_ram + total_vram) * 2500)\n",
    "    return max(min(estimated_chunk_size, max_chunk_size), min_chunk_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbd9e9-09c9-4f33-a36c-857229173772",
   "metadata": {},
   "source": [
    "<b>–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957916c7-e0f5-480e-b4e2-2d35602a6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output_final_4persent.csv\", sep=\";\").dropna()\n",
    "\n",
    "# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏\n",
    "if \"text_wich_errors\" in df.columns:\n",
    "    df.rename(columns={\"text_wich_errors\": \"text_with_errors\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60604cbf-2647-4b34-8933-bc9415e695fa",
   "metadata": {},
   "source": [
    "<b>–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –∏ —á–∞–Ω–∫–æ–≤<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20978e7c-f08a-465b-bb8c-8efa26dd0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ß–∞–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./t5-spell-corrector-v2\"  # –ü–∞–ø–∫–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)  # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
    "\n",
    "chunk_size = get_optimal_chunk_size()\n",
    "chunk_files = sorted(glob.glob(\"chunk_v2_*.csv\"))  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —á–∞–Ω–∫–∏\n",
    "\n",
    "# –ï—Å–ª–∏ —á–∞–Ω–∫–æ–≤ –Ω–µ—Ç, —Å–æ–∑–¥–∞—ë–º –∑–∞–Ω–æ–≤–æ\n",
    "if chunk_files:\n",
    "    print(\"‚úÖ –ß–∞–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\")\n",
    "else:\n",
    "    print(\"üîπ –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ...\")\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        df.iloc[start:start + chunk_size].to_csv(f\"chunk_v2_{i + 1}.csv\", sep=\";\", index=False)\n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {i + 1} –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c26070-1525-47cc-a4d8-11ee44fcfe96",
   "metadata": {},
   "source": [
    "<b>–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064904c7-c640-4057-98a1-ad1bab759383",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UrukHan/t5-russian-spell\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 8-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "model.config.use_cache = False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcf34e-bb2f-4412-b04f-635081c6fea1",
   "metadata": {},
   "source": [
    "<b>–§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616005bf-e04d-485e-8e42-fbd255004a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    labels = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6a752-eaff-4139-9361-838a605d809b",
   "metadata": {},
   "source": [
    "<b>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LoRA<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd1d931-59a2-4892-9278-600ebbc94dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1e8c5-d81a-451d-a96e-a1335eef59cf",
   "metadata": {},
   "source": [
    "<b>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298799a4-3136-43b4-aae0-b52319ce9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_dir,  # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –Ω–æ–≤–æ–π –ø–∞–ø–∫–µ\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb53fc-e8b6-4955-b4a6-9e6511286885",
   "metadata": {},
   "source": [
    "<b>–§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57df10ce-4ffa-433d-a8d8-5a77d06dc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: ./t5-spell-corrector-v2/checkpoint-3700\n"
     ]
    }
   ],
   "source": [
    "def find_last_valid_checkpoint():\n",
    "    checkpoint_list = sorted(\n",
    "        glob.glob(f\"{checkpoint_dir}/checkpoint-*\"),\n",
    "        key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)) if re.search(r'checkpoint-(\\d+)', x) else 0\n",
    "    )\n",
    "    for checkpoint in reversed(checkpoint_list):\n",
    "        if os.path.exists(os.path.join(checkpoint, \"trainer_state.json\")):\n",
    "            return checkpoint  \n",
    "    return None  \n",
    "\n",
    "last_checkpoint = find_last_valid_checkpoint()\n",
    "\n",
    "if last_checkpoint:\n",
    "    print(f\"‚úÖ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {last_checkpoint}\")\n",
    "else:\n",
    "    print(\"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω—É–ª—è.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a4aea-ca21-45c5-a88e-dbb0ac2dc095",
   "metadata": {},
   "source": [
    "<b>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∞—Ö<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e9b4e-9515-4e53-95e8-ca79b7efaa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º —á–∞–Ω–∫–µ 1/5: chunk_v2_1.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9fce3f8c154d4481a900b00fd34884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9154e564e9884f7f8a2c7e6e94ae1acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1332911d7454f33915e8d49a50851ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d67b8205164aee8543ca24b56c4ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å ./t5-spell-corrector-v2/checkpoint-3700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5801' max='8436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5801/8436 9:13:44 < 11:35:08, 0.06 it/s, Epoch 2.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.023476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.023346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.023295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.023198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.023101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.023018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.022916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.022855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.022786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.022717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.022613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.022584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.022549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.022552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.022369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.022418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.022311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.022279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1922' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1922/5000 06:22 < 10:12, 5.03 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_files = sorted(glob.glob(\"chunk_v2_*.csv\"))\n",
    "\n",
    "for i, chunk_file in enumerate(chunk_files):\n",
    "    print(f\"‚ñ∂Ô∏è –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º —á–∞–Ω–∫–µ {i+1}/{len(chunk_files)}: {chunk_file}\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    df_chunk = pd.read_csv(chunk_file, sep=\";\")\n",
    "    dataset = Dataset.from_pandas(df_chunk).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    dataset = dataset.map(lambda x: {\"input_text\": \"–ò—Å–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç: \" + x[\"text_with_errors\"], \"target_text\": x[\"corrected_text\"]})\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    if \"input_text\" in dataset[\"train\"].column_names and \"target_text\" in dataset[\"train\"].column_names:\n",
    "        dataset = dataset.remove_columns([\"input_text\", \"target_text\"])\n",
    "\n",
    "    # –°–æ–∑–¥–∞—ë–º Trainer\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], data_collator=DataCollatorForSeq2Seq(tokenizer, model=model))\n",
    "\n",
    "    # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "    if last_checkpoint:\n",
    "        print(f\"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {last_checkpoint}\")\n",
    "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    else:\n",
    "        print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è...\")\n",
    "        trainer.train()\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "new_checkpoint = f\"{checkpoint_dir}/checkpoint-{i+1}\"\n",
    "trainer.save_model(new_checkpoint)\n",
    "\n",
    "# üîπ –û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ GPU\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª—è–µ–º last_checkpoint\n",
    "last_checkpoint = new_checkpoint\n",
    "\n",
    "\n",
    "print(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ù–æ–≤—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:\", checkpoint_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
