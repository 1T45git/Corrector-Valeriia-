{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae5de00-f250-4b5e-a52a-cecb3ebba49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft bitsandbytes torch accelerate sentencepiece pandas psutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce37b25-140d-426c-8dd1-2f0c1b8b4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, \n",
    "    DataCollatorForSeq2Seq, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba294213-38e1-46da-8f4f-f5b20a437992",
   "metadata": {},
   "source": [
    "<b>–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞ <b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3623d87-568e-4040-8eea-6578ef3a7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_chunk_size(min_chunk_size=10000, max_chunk_size=50000):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π RAM –∏ VRAM.\n",
    "    \"\"\"\n",
    "    total_ram = psutil.virtual_memory().available / (1024 ** 3)  # –î–æ—Å—Ç—É–ø–Ω–∞—è RAM\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if torch.cuda.is_available() else 0  # VRAM\n",
    "    estimated_chunk_size = int((total_ram + total_vram) * 2500)\n",
    "    return max(min(estimated_chunk_size, max_chunk_size), min_chunk_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84779e3-6f53-498a-9cb0-749d4c283151",
   "metadata": {},
   "source": [
    "<b>–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9640066-4614-4f81-a0f8-58ed8c590d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ß–∞–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"output_final_4persent.csv\", sep=\";\").dropna()  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "\n",
    "if \"text_wich_errors\" in df.columns:\n",
    "    df.rename(columns={\"text_wich_errors\": \"text_with_errors\"}, inplace=True)  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞\n",
    "\n",
    "chunk_size = get_optimal_chunk_size()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —á–∞–Ω–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏—Ö –ø–æ–≤—Ç–æ—Ä–Ω–æ\n",
    "if not any(fname.startswith(\"chunk_\") for fname in os.listdir(\".\")):\n",
    "    print(\"üîπ –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ...\")\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        df.iloc[start:start + chunk_size].to_csv(f\"chunk_{i + 1}.csv\", sep=\";\", index=False)\n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {i + 1} —á–∞–Ω–∫–æ–≤.\")\n",
    "else:\n",
    "    print(\"‚úÖ –ß–∞–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dfb1b-88e2-4020-b850-9455a4a53abf",
   "metadata": {},
   "source": [
    "<b>–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95a240b-73f3-448c-89cb-f45914364938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UrukHan/t5-russian-spell\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "model.config.use_cache = False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcf391-8843-4c0d-a18d-fc8be1184c06",
   "metadata": {},
   "source": [
    "<b>–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380a45fc-5b74-495e-9f86-aedbb85609fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã, —Å–æ–∑–¥–∞–≤–∞—è labels –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39554c83-299b-4564-a0df-8dd2232bb786",
   "metadata": {},
   "source": [
    "<b>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd43cd80-ce60-4691-8409-9851951a81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)  # –î–æ–±–∞–≤–ª—è–µ–º LoRA\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad11551-0949-408f-837d-eeeb8c6595e2",
   "metadata": {},
   "source": [
    "<b>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84b4bd5-53e2-465d-a9ae-253985686e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-spell-corrector\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    weight_decay=0.01,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c68060-d3eb-47a6-a69f-90fb0232b422",
   "metadata": {},
   "source": [
    "<b>–ü–æ–∏—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤<b/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "520663c7-954b-44e7-b0a3-92f0591627c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —á–∞–Ω–∫: 5\n",
      "‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–±–æ—á–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: ./t5-spell-corrector/checkpoint-8436\n",
      "üîÑ –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è —Å —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏...\n"
     ]
    }
   ],
   "source": [
    "# –£–∫–∞–∑—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
    "checkpoint_dir = \"./t5-spell-corrector\"\n",
    "\n",
    "import json\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è: –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —á–∞–Ω–∫\n",
    "def find_last_processed_chunk():\n",
    "    chunk_numbers = []\n",
    "    for folder in glob.glob(f\"{checkpoint_dir}/checkpoint-*\"):\n",
    "        match = re.search(r'checkpoint-(\\d+)', folder)\n",
    "        if match:\n",
    "            step_number = int(match.group(1))  # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "            chunk_number = min(step_number // 100 + 1, 5)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —á–∞–Ω–∫–æ–≤\n",
    "            chunk_numbers.append(chunk_number)\n",
    "    return max(chunk_numbers) if chunk_numbers else 0\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è: –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞–ª–∏–¥–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "def find_last_valid_checkpoint():\n",
    "    checkpoint_list = sorted(\n",
    "        glob.glob(f\"{checkpoint_dir}/checkpoint-*\"),\n",
    "        key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)) if re.search(r'checkpoint-(\\d+)', x) else 0\n",
    "    )\n",
    "    for checkpoint in reversed(checkpoint_list):\n",
    "        trainer_state_path = os.path.join(checkpoint, \"trainer_state.json\")\n",
    "        if os.path.exists(trainer_state_path):\n",
    "            return checkpoint  \n",
    "    return None  \n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –ª–∏ –≤—Å–µ —ç–ø–æ—Ö–∏\n",
    "def check_if_training_completed(last_checkpoint):\n",
    "    trainer_state_path = os.path.join(last_checkpoint, \"trainer_state.json\")\n",
    "    \n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "            current_epoch = state.get(\"epoch\", None)\n",
    "            total_epochs = training_args.num_train_epochs\n",
    "\n",
    "            if current_epoch is not None and current_epoch >= total_epochs:\n",
    "                return True  # –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n",
    "    return False  # –ù—É–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "# üîπ –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —á–∞–Ω–∫ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "last_processed_chunk = find_last_processed_chunk()\n",
    "last_checkpoint = find_last_valid_checkpoint()\n",
    "\n",
    "# üîπ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –ª–∏ –æ–±—É—á–µ–Ω–∏–µ\n",
    "training_completed = check_if_training_completed(last_checkpoint)\n",
    "\n",
    "print(f\"üìå –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —á–∞–Ω–∫: {last_processed_chunk}\")\n",
    "if last_checkpoint:\n",
    "    print(f\"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–±–æ—á–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {last_checkpoint}\")\n",
    "    if training_completed:\n",
    "        print(\"üéâ –û–±—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ! üöÄ\")\n",
    "    else:\n",
    "        print(\"üîÑ –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è —Å —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏...\")\n",
    "else:\n",
    "    print(\"‚ùå –†–∞–±–æ—á–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –æ–±—É—á–µ–Ω–∏–µ –Ω–∞—á–Ω–µ—Ç—Å—è —Å –Ω—É–ª—è.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8ca3a-56b6-4a59-ba24-2aa87ca54f16",
   "metadata": {},
   "source": [
    "<b>–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5e0105-7976-4d70-91dc-c120d3b4241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ 1, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n",
      "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ 2, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n",
      "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ 3, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n",
      "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ 4, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n",
      "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ 5, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n"
     ]
    }
   ],
   "source": [
    "chunk_files = sorted(glob.glob(\"chunk_*.csv\"))\n",
    "\n",
    "if not chunk_files:\n",
    "    raise FileNotFoundError(\"‚ùå –û—à–∏–±–∫–∞: –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã `chunk_*.csv` —Å—É—â–µ—Å—Ç–≤—É—é—Ç.\")\n",
    "\n",
    "last_processed_chunk = int(re.findall(r'\\d+', last_checkpoint)[-1]) if last_checkpoint else 0\n",
    "\n",
    "for i, chunk_file in enumerate(chunk_files):\n",
    "    current_chunk_number = i + 1  \n",
    "\n",
    "    if current_chunk_number <= last_processed_chunk:\n",
    "        print(f\"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ {current_chunk_number}, –æ–Ω —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–µ {current_chunk_number}/{len(chunk_files)}: {chunk_file}\")\n",
    "\n",
    "    df_chunk = pd.read_csv(chunk_file, sep=\";\")\n",
    "    dataset = Dataset.from_pandas(df_chunk).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    dataset = dataset.map(lambda x: {\"input_text\": \"–ò—Å–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç: \" + x[\"text_with_errors\"], \"target_text\": x[\"corrected_text\"]})\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], data_collator=data_collator)\n",
    "\n",
    "    if last_checkpoint and os.path.exists(os.path.join(last_checkpoint, \"trainer_state.json\")):\n",
    "        print(f\"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {last_checkpoint}\")\n",
    "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    else:\n",
    "        print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è\")\n",
    "        trainer.train()\n",
    "\n",
    "    new_checkpoint = f\"{checkpoint_dir}/checkpoint-{current_chunk_number}\"\n",
    "    trainer.save_model(new_checkpoint)\n",
    "    last_checkpoint = new_checkpoint\n",
    "    print(f\"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç: {new_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "106a6007-2a61-49a4-85a4-52d9ee3378bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rich  \n",
    "from rich.console import Console\n",
    "from rich.markup import escape\n",
    "import difflib\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π\n",
    "def highlight_changes(original, corrected):\n",
    "    diff = difflib.ndiff(original.split(), corrected.split())\n",
    "    highlighted_text = []\n",
    "\n",
    "    for word in diff:\n",
    "        if word.startswith(\"- \"):  # –£–¥–∞–ª—ë–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (–æ—à–∏–±–∫–∞)\n",
    "            highlighted_text.append(f\"[red]{escape(word[2:])}[/red]\")\n",
    "        elif word.startswith(\"+ \"):  # –î–æ–±–∞–≤–ª–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)\n",
    "            highlighted_text.append(f\"[green]{escape(word[2:])}[/green]\")\n",
    "        else:\n",
    "            highlighted_text.append(word[2:])\n",
    "\n",
    "    return \" \".join(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ef2120-ee22-42b0-9679-1aaed061fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ ./t5-spell-corrector/checkpoint-8436\n",
      "‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ UrukHan/t5-russian-spell\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# –£–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "checkpoint_path = \"./t5-spell-corrector/checkpoint-8436\"  # –£–∫–∞–∂–∏ —Ä–µ–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "original_model_name = \"UrukHan/t5-russian-spell\"  # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —ç—Ç–æ —Ç–∞ –∂–µ –º–æ–¥–µ–ª—å\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {checkpoint_path}\")\n",
    "print(f\"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {original_model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceded6a4-b73c-43b5-b34e-074d7c368a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(input_text):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –æ—à–∏–±–∫–∞–º–∏, –ø–µ—Ä–µ–¥–∞—ë—Ç –µ–≥–æ –≤ –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "    \"\"\"\n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏\n",
    "    input_text = \"–ò—Å–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç: \" + input_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "\n",
    "    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe1ae556-c85b-42cd-a26c-d8e21bfa7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª: —Å–µ–≥–ª–¥—ã—è —Ö–æ—Ä–æ—à –¥–µ–Ω\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: ¬´–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à –¥–µ–Ω—å.\n",
      "\n",
      "‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª: —è –Ω–≤–∏–¥–µ–ª —Ç–∞–±—è\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –Ø –Ω–µ –≤–∏–¥–µ–ª —Ç–µ–±—è.\n",
      "\n",
      "‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª: –ø—Ä–µ–≤–µ—Ç –∫–∞–∫ –¥–∏–ª–∞\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: ¬´–û—Ç–≤–µ—Ç –∫–∞–∫ –¥–∏–ª–∞¬ª\n",
      "\n",
      "‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª: –¥–µ–≤–æ—á–∫–∞ –∏—à–ª–∞ –¥–æ –º–∞–≥–∏–∑–∏–Ω–∞\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –î–µ–≤–æ—á–∫–∞ –∏—à–ª–∞ –¥–æ –º–∞–≥–∏–∑–∏–Ω–∞.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –æ—à–∏–±–∫–∞–º–∏\n",
    "test_texts = [\"—Å–µ–≥–ª–¥—ã—è —Ö–æ—Ä–æ—à –¥–µ–Ω\",  # —Å–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∏–π –¥–µ–Ω—å\n",
    "    \"—è –Ω–≤–∏–¥–µ–ª —Ç–∞–±—è\",      # —è –≤–∏–¥–µ–ª —Ç–µ–±—è\n",
    "    \"–ø—Ä–µ–≤–µ—Ç –∫–∞–∫ –¥–∏–ª–∞\",    # –ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞\n",
    "    \"–¥–µ–≤–æ—á–∫–∞ –∏—à–ª–∞ –¥–æ –º–∞–≥–∏–∑–∏–Ω–∞\"  # –¥–µ–≤–æ—á–∫–∞ —à–ª–∞ –≤ –º–∞–≥–∞–∑–∏–Ω\n",
    "]\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫ –º–æ–¥–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç\n",
    "for text in test_texts:\n",
    "    corrected = correct_text(text)\n",
    "    print(f\"‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª: {text}\")\n",
    "    print(f\"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {corrected}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa387b-10fb-41c0-ba13-512e6c9d8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe198fe-bcef-482b-bd76-0518f995debd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
