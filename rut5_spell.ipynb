{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae5de00-f250-4b5e-a52a-cecb3ebba49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft bitsandbytes torch accelerate sentencepiece pandas psutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce37b25-140d-426c-8dd1-2f0c1b8b4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, \n",
    "    DataCollatorForSeq2Seq, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba294213-38e1-46da-8f4f-f5b20a437992",
   "metadata": {},
   "source": [
    "<b>Функция для вычисления оптимального размера чанка <b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3623d87-568e-4040-8eea-6578ef3a7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_chunk_size(min_chunk_size=10000, max_chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Рассчитывает оптимальный размер чанка на основе доступной RAM и VRAM.\n",
    "    \"\"\"\n",
    "    total_ram = psutil.virtual_memory().available / (1024 ** 3)  # Доступная RAM\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if torch.cuda.is_available() else 0  # VRAM\n",
    "    estimated_chunk_size = int((total_ram + total_vram) * 2500)\n",
    "    return max(min(estimated_chunk_size, max_chunk_size), min_chunk_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84779e3-6f53-498a-9cb0-749d4c283151",
   "metadata": {},
   "source": [
    "<b>Загрузка датасета и его обработка<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9640066-4614-4f81-a0f8-58ed8c590d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Чанки уже существуют, повторное создание не требуется.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"output_final_4persent.csv\", sep=\";\").dropna()  # Загружаем и очищаем датасет\n",
    "\n",
    "if \"text_wich_errors\" in df.columns:\n",
    "    df.rename(columns={\"text_wich_errors\": \"text_with_errors\"}, inplace=True)  # Исправляем название столбца\n",
    "\n",
    "chunk_size = get_optimal_chunk_size()\n",
    "\n",
    "# Проверяем, есть ли уже чанки, чтобы не создавать их повторно\n",
    "if not any(fname.startswith(\"chunk_\") for fname in os.listdir(\".\")):\n",
    "    print(\"🔹 Чанки не найдены, создаем заново...\")\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        df.iloc[start:start + chunk_size].to_csv(f\"chunk_{i + 1}.csv\", sep=\";\", index=False)\n",
    "    print(f\"✅ Датасет разбит на {i + 1} чанков.\")\n",
    "else:\n",
    "    print(\"✅ Чанки уже существуют, повторное создание не требуется.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dfb1b-88e2-4020-b850-9455a4a53abf",
   "metadata": {},
   "source": [
    "<b>Загрузка модели и токенизатора<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95a240b-73f3-448c-89cb-f45914364938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"UrukHan/t5-russian-spell\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "model.config.use_cache = False  # Отключаем кеширование\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcf391-8843-4c0d-a18d-fc8be1184c06",
   "metadata": {},
   "source": [
    "<b>Функция для токенизации данных<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380a45fc-5b74-495e-9f86-aedbb85609fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Токенизирует входные и целевые тексты, создавая labels для обучения.\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39554c83-299b-4564-a0df-8dd2232bb786",
   "metadata": {},
   "source": [
    "<b>Настройка LoRA и обновление модели<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd43cd80-ce60-4691-8409-9851951a81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)  # Добавляем LoRA\n",
    "\n",
    "# Включаем градиенты только для LoRA-адаптеров\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad11551-0949-408f-837d-eeeb8c6595e2",
   "metadata": {},
   "source": [
    "<b>Настройка параметров обучения<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84b4bd5-53e2-465d-a9ae-253985686e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-spell-corrector\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    weight_decay=0.01,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c68060-d3eb-47a6-a69f-90fb0232b422",
   "metadata": {},
   "source": [
    "<b>Поиск сохраненных чекпоинтов<b/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "520663c7-954b-44e7-b0a3-92f0591627c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Последний обработанный чанк: 5\n",
      "✅ Обнаружен последний рабочий чекпоинт: ./t5-spell-corrector/checkpoint-8436\n",
      "🔄 Обучение продолжается с текущей эпохи...\n"
     ]
    }
   ],
   "source": [
    "# Указываем директорию для чекпоинтов\n",
    "checkpoint_dir = \"./t5-spell-corrector\"\n",
    "\n",
    "import json\n",
    "\n",
    "# Функция: Найти последний обработанный чанк\n",
    "def find_last_processed_chunk():\n",
    "    chunk_numbers = []\n",
    "    for folder in glob.glob(f\"{checkpoint_dir}/checkpoint-*\"):\n",
    "        match = re.search(r'checkpoint-(\\d+)', folder)\n",
    "        if match:\n",
    "            step_number = int(match.group(1))  # Получаем номер чекпоинта\n",
    "            chunk_number = min(step_number // 100 + 1, 5)  # Ограничиваем до 5 чанков\n",
    "            chunk_numbers.append(chunk_number)\n",
    "    return max(chunk_numbers) if chunk_numbers else 0\n",
    "\n",
    "# Функция: Найти последний валидный чекпоинт\n",
    "def find_last_valid_checkpoint():\n",
    "    checkpoint_list = sorted(\n",
    "        glob.glob(f\"{checkpoint_dir}/checkpoint-*\"),\n",
    "        key=lambda x: int(re.search(r'checkpoint-(\\d+)', x).group(1)) if re.search(r'checkpoint-(\\d+)', x) else 0\n",
    "    )\n",
    "    for checkpoint in reversed(checkpoint_list):\n",
    "        trainer_state_path = os.path.join(checkpoint, \"trainer_state.json\")\n",
    "        if os.path.exists(trainer_state_path):\n",
    "            return checkpoint  \n",
    "    return None  \n",
    "\n",
    "# Функция: Проверить, завершились ли все эпохи\n",
    "def check_if_training_completed(last_checkpoint):\n",
    "    trainer_state_path = os.path.join(last_checkpoint, \"trainer_state.json\")\n",
    "    \n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "            current_epoch = state.get(\"epoch\", None)\n",
    "            total_epochs = training_args.num_train_epochs\n",
    "\n",
    "            if current_epoch is not None and current_epoch >= total_epochs:\n",
    "                return True  # Обучение завершено\n",
    "    return False  # Нужно продолжить обучение\n",
    "\n",
    "# 🔹 Определяем последний обработанный чанк и чекпоинт\n",
    "last_processed_chunk = find_last_processed_chunk()\n",
    "last_checkpoint = find_last_valid_checkpoint()\n",
    "\n",
    "# 🔹 Проверяем, завершилось ли обучение\n",
    "training_completed = check_if_training_completed(last_checkpoint)\n",
    "\n",
    "print(f\"📌 Последний обработанный чанк: {last_processed_chunk}\")\n",
    "if last_checkpoint:\n",
    "    print(f\"✅ Обнаружен последний рабочий чекпоинт: {last_checkpoint}\")\n",
    "    if training_completed:\n",
    "        print(\"🎉 Обучение полностью завершено! 🚀\")\n",
    "    else:\n",
    "        print(\"🔄 Обучение продолжается с текущей эпохи...\")\n",
    "else:\n",
    "    print(\"❌ Рабочие чекпоинты не найдены, обучение начнется с нуля.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8ca3a-56b6-4a59-ba24-2aa87ca54f16",
   "metadata": {},
   "source": [
    "<b>Запуск обучения<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5e0105-7976-4d70-91dc-c120d3b4241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ Пропускаем чанк 1, он уже обработан.\n",
      "⏭️ Пропускаем чанк 2, он уже обработан.\n",
      "⏭️ Пропускаем чанк 3, он уже обработан.\n",
      "⏭️ Пропускаем чанк 4, он уже обработан.\n",
      "⏭️ Пропускаем чанк 5, он уже обработан.\n"
     ]
    }
   ],
   "source": [
    "chunk_files = sorted(glob.glob(\"chunk_*.csv\"))\n",
    "\n",
    "if not chunk_files:\n",
    "    raise FileNotFoundError(\"❌ Ошибка: Чанки не найдены! Убедись, что файлы `chunk_*.csv` существуют.\")\n",
    "\n",
    "last_processed_chunk = int(re.findall(r'\\d+', last_checkpoint)[-1]) if last_checkpoint else 0\n",
    "\n",
    "for i, chunk_file in enumerate(chunk_files):\n",
    "    current_chunk_number = i + 1  \n",
    "\n",
    "    if current_chunk_number <= last_processed_chunk:\n",
    "        print(f\"⏭️ Пропускаем чанк {current_chunk_number}, он уже обработан.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"▶️ Обучение на чанке {current_chunk_number}/{len(chunk_files)}: {chunk_file}\")\n",
    "\n",
    "    df_chunk = pd.read_csv(chunk_file, sep=\";\")\n",
    "    dataset = Dataset.from_pandas(df_chunk).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    dataset = dataset.map(lambda x: {\"input_text\": \"Исправь текст: \" + x[\"text_with_errors\"], \"target_text\": x[\"corrected_text\"]})\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], data_collator=data_collator)\n",
    "\n",
    "    if last_checkpoint and os.path.exists(os.path.join(last_checkpoint, \"trainer_state.json\")):\n",
    "        print(f\"🔄 Продолжаем обучение с последнего рабочего чекпоинта: {last_checkpoint}\")\n",
    "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "    else:\n",
    "        print(\"🚀 Начинаем обучение с нуля\")\n",
    "        trainer.train()\n",
    "\n",
    "    new_checkpoint = f\"{checkpoint_dir}/checkpoint-{current_chunk_number}\"\n",
    "    trainer.save_model(new_checkpoint)\n",
    "    last_checkpoint = new_checkpoint\n",
    "    print(f\"✅ Сохранён новый чекпоинт: {new_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "106a6007-2a61-49a4-85a4-52d9ee3378bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rich  \n",
    "from rich.console import Console\n",
    "from rich.markup import escape\n",
    "import difflib\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Функция для подсветки исправлений\n",
    "def highlight_changes(original, corrected):\n",
    "    diff = difflib.ndiff(original.split(), corrected.split())\n",
    "    highlighted_text = []\n",
    "\n",
    "    for word in diff:\n",
    "        if word.startswith(\"- \"):  # Удалённое слово (ошибка)\n",
    "            highlighted_text.append(f\"[red]{escape(word[2:])}[/red]\")\n",
    "        elif word.startswith(\"+ \"):  # Добавленное слово (исправление)\n",
    "            highlighted_text.append(f\"[green]{escape(word[2:])}[/green]\")\n",
    "        else:\n",
    "            highlighted_text.append(word[2:])\n",
    "\n",
    "    return \" \".join(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ef2120-ee22-42b0-9679-1aaed061fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Модель загружена из ./t5-spell-corrector/checkpoint-8436\n",
      "✅ Токенизатор загружен из UrukHan/t5-russian-spell\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Указываем последний чекпоинт\n",
    "checkpoint_path = \"./t5-spell-corrector/checkpoint-8436\"  # Укажи реальный номер\n",
    "\n",
    "# Загружаем токенизатор из исходной модели, а не из чекпоинта\n",
    "original_model_name = \"UrukHan/t5-russian-spell\"  # Убедись, что это та же модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "\n",
    "# Загружаем модель из чекпоинта\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"✅ Модель загружена из {checkpoint_path}\")\n",
    "print(f\"✅ Токенизатор загружен из {original_model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceded6a4-b73c-43b5-b34e-074d7c368a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(input_text):\n",
    "    \"\"\"\n",
    "    Функция принимает текст с ошибками, передаёт его в модель и возвращает исправленный текст.\n",
    "    \"\"\"\n",
    "    # Формируем запрос к модели\n",
    "    input_text = \"Исправь текст: \" + input_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "\n",
    "    # Генерация исправленного текста\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "\n",
    "    # Декодируем результат\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe1ae556-c85b-42cd-a26c-d8e21bfa7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Оригинал: сеглдыя хорош ден\n",
      "✅ Исправлено: «Сегодня хорош день.\n",
      "\n",
      "❌ Оригинал: я нвидел табя\n",
      "✅ Исправлено: Я не видел тебя.\n",
      "\n",
      "❌ Оригинал: превет как дила\n",
      "✅ Исправлено: «Ответ как дила»\n",
      "\n",
      "❌ Оригинал: девочка ишла до магизина\n",
      "✅ Исправлено: Девочка ишла до магизина.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример текста с ошибками\n",
    "test_texts = [\"сеглдыя хорош ден\",  # сегодня хороший день\n",
    "    \"я нвидел табя\",      # я видел тебя\n",
    "    \"превет как дила\",    # привет как дела\n",
    "    \"девочка ишла до магизина\"  # девочка шла в магазин\n",
    "]\n",
    "\n",
    "# Проверяем, как модель исправляет текст\n",
    "for text in test_texts:\n",
    "    corrected = correct_text(text)\n",
    "    print(f\"❌ Оригинал: {text}\")\n",
    "    print(f\"✅ Исправлено: {corrected}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa387b-10fb-41c0-ba13-512e6c9d8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe198fe-bcef-482b-bd76-0518f995debd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
